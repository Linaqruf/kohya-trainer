{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/experimental/kohya-dreambooth.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Kohya Dreambooth V18 - VRAM 12GB"
      ],
      "metadata": {
        "id": "slgjeYgd6pWp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adapted to Google Colab based on [Kohya Guide](https://note.com/kohya_ss/n/nee3ed1649fb6)<br>\n",
        "Adapted again from [bmaltais's Kohya Archive](https://github.com/bmaltais/kohya_ss)<br>\n",
        "Adapted to Google Colab by [Linaqruf](https://github.com/Linaqruf)<br>\n",
        "You can find latest notebook update [here](https://github.com/Linaqruf/kohya-trainer/blob/main/kohya-dreambooth.ipynb)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gPgBR3KM6E-Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Install Kohya Trainer"
      ],
      "metadata": {
        "id": "tTVqCAgSmie4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_u3q60di584x",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Clone Kohya Trainer\n",
        "#@markdown Clone the Kohya Trainer repository from GitHub and check for updates\n",
        "\n",
        "%cd /content/\n",
        "\n",
        "import os\n",
        "\n",
        "def clone_kohya_trainer():\n",
        "  # Check if the directory already exists\n",
        "  if os.path.isdir('/content/kohya-trainer'):\n",
        "    %cd /content/kohya-trainer\n",
        "    print(\"This folder already exists, will do a !git pull instead\\n\")\n",
        "    !git pull\n",
        "  else:\n",
        "    !git clone https://github.com/Linaqruf/kohya-trainer\n",
        "\n",
        "# Clone or update the Kohya Trainer repository\n",
        "clone_kohya_trainer()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Installing Dependencies\n",
        "%cd /content/kohya-trainer\n",
        "\n",
        "import os\n",
        "\n",
        "def install_dependencies():\n",
        "  #@markdown This will install required Python packages\n",
        "  !pip install --upgrade -r script/requirements.txt\n",
        "  !pip install -U gallery-dl\n",
        "  !pip install huggingface_hub\n",
        "  \n",
        "  Install_xformers = True #@param {'type':'boolean'}\n",
        "  \n",
        "  if Install_xformers:\n",
        "    !pip install -U -I --no-deps https://github.com/camenduru/stable-diffusion-webui-colab/releases/download/0.0.15/xformers-0.0.15.dev0+189828c.d20221207-cp38-cp38-linux_x86_64.whl\n",
        "  else:\n",
        "    pass\n",
        "\n",
        "\n",
        "# Install dependencies\n",
        "install_dependencies()\n",
        "\n",
        "#@markdown After Accelerate updated its version to 0.15.0, you can't manually input the config using\n",
        "#@markdown `!accelerate config` in Google Colab. Instead, a `config.yaml` file will be generated by\n",
        "#@markdown the `write_basic_config()` function. You can find the file [here](/content/kohya-trainer/accelerate_config/config.yaml) after installation.\n",
        "#@markdown if you want to modify it.\n",
        "from accelerate.utils import write_basic_config\n",
        "\n",
        "accelerate_config = \"/content/kohya-trainer/accelerate_config/config.yaml\"\n",
        "write_basic_config(save_location = accelerate_config) # Write a config file"
      ],
      "metadata": {
        "id": "WNn0g1pnHfk5",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install Pre-trained Model \n",
        "%cd /content/kohya-trainer\n",
        "import os\n",
        "\n",
        "# Check if directory exists\n",
        "if not os.path.exists('checkpoint'):\n",
        "  # Create directory if it doesn't exist\n",
        "  os.makedirs('checkpoint')\n",
        "\n",
        "#@title Install Pre-trained Model \n",
        "\n",
        "installModels=[]\n",
        "installVae= []\n",
        "installVaeArgs = []\n",
        "\n",
        "#@markdown ### Available Model\n",
        "#@markdown Select one of available pretrained model to download:\n",
        "modelUrl = [\"\", \\\n",
        "            \"https://huggingface.co/Linaqruf/personal_backup/resolve/main/animeckpt/model-pruned.ckpt\", \\\n",
        "            \"https://huggingface.co/Linaqruf/personal_backup/resolve/main/animeckpt/modelsfw-pruned.ckpt\", \\\n",
        "            \"https://huggingface.co/Linaqruf/anything-v3.0/resolve/main/Anything-V3.0-pruned-fp16.ckpt\", \\\n",
        "            \"https://huggingface.co/Linaqruf/anything-v3.0/resolve/main/Anything-V3.0-pruned-fp32.ckpt\", \\\n",
        "            \"https://huggingface.co/Linaqruf/anything-v3.0/resolve/main/Anything-V3.0-pruned.ckpt\", \\\n",
        "            \"https://huggingface.co/CompVis/stable-diffusion-v-1-4-original/resolve/main/sd-v1-4.ckpt\", \\\n",
        "            \"https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.ckpt\", \\\n",
        "            \"https://huggingface.co/hakurei/waifu-diffusion-v1-3/resolve/main/wd-v1-3-float32.ckpt\"]\n",
        "modelList = [\"\", \\\n",
        "             \"Animefull-final-pruned\", \\\n",
        "             \"Animesfw-final-pruned\", \\\n",
        "             \"Anything-V3.0-pruned-fp16\", \\\n",
        "             \"Anything-V3.0-pruned-fp32\", \\\n",
        "             \"Anything-V3.0-pruned\", \\\n",
        "             \"Stable-Diffusion-v1-4\", \\\n",
        "             \"Stable-Diffusion-v1-5-pruned-emaonly\", \\\n",
        "             \"Waifu-Diffusion-v1-3-fp32\"]\n",
        "modelName = \"Anything-V3.0-pruned\" #@param [\"\", \"Animefull-final-pruned\", \"Animesfw-final-pruned\", \"Anything-V3.0-pruned-fp16\", \"Anything-V3.0-pruned-fp32\", \"Anything-V3.0-pruned\", \"Stable-Diffusion-v1-4\", \"Stable-Diffusion-v1-5-pruned-emaonly\", \"Waifu-Diffusion-v1-3-fp32\"]\n",
        "\n",
        "#@markdown ### Custom model\n",
        "#@markdown The model URL should be a direct download link.\n",
        "customName = \"\" #@param {'type': 'string'}\n",
        "customUrl = \"\"#@param {'type': 'string'}\n",
        "\n",
        "\n",
        "# Check if user has specified a custom model\n",
        "if customName != \"\" and customUrl != \"\":\n",
        "  # Add custom model to list of models to install\n",
        "  installModels.append((customName, customUrl))\n",
        "\n",
        "# Check if user has selected a model\n",
        "if modelName != \"\":\n",
        "  # Map selected model to URL\n",
        "  installModels.append((modelName, modelUrl[modelList.index(modelName)]))\n",
        "\n",
        "#@markdown Select one of the VAEs to download, select `none` for not download VAE:\n",
        "vaeUrl = [\"\", \\\n",
        "          \"https://huggingface.co/Linaqruf/personal_backup/resolve/main/animevae/animevae.pt\", \\\n",
        "          \"https://huggingface.co/hakurei/waifu-diffusion-v1-4/resolve/main/vae/kl-f8-anime.ckpt\"]\n",
        "vaeList = [\"none\", \\\n",
        "           \"anime.vae.pt\", \\\n",
        "           \"waifudiffusion.vae.pt\"]\n",
        "vaeName = \"anime.vae.pt\" #@param [\"none\",\"anime.vae.pt\",\"waifudiffusion.vae.pt\"]\n",
        "vae_args = [\"none\", \\\n",
        "            \"--vae-path /content/stable-diffusion-webui/models/Stable-diffusion/anime.vae.pt\", \\\n",
        "            \"--vae-path /content/stable-diffusion-webui/models/Stable-diffusion/waifudiffusion.vae.pt\"]\n",
        "\n",
        "installVae.append((vaeName, vaeUrl[vaeList.index(vaeName)]))\n",
        "installVaeArgs.append((vae_args[vaeList.index(vaeName)]))\n",
        "\n",
        "def install_aria():\n",
        "  # Install aria2 if it is not already installed\n",
        "  if not os.path.exists('/usr/bin/aria2c'):\n",
        "    !apt install -y -qq aria2\n",
        "\n",
        "def install(checkpoint_name, url):\n",
        "  if url.endswith(\".ckpt\"):\n",
        "    dst = \"/content/kohya-trainer/checkpoint/\" + str(checkpoint_name) + \".ckpt\"\n",
        "  elif url.endswith(\".safetensors\"):\n",
        "    dst = \"/content/kohya-trainer/checkpoint/\" + str(checkpoint_name) + \".safetensors\"\n",
        "  elif url.endswith(\".pt\"):\n",
        "    dst = \"/content/kohya-trainer/checkpoint/\" + str(checkpoint_name)\n",
        "  else:\n",
        "    dst = \"/content/kohya-trainer/checkpoint/\" + str(checkpoint_name) + \".ckpt\"\n",
        "\n",
        "  if url.startswith(\"https://drive.google.com\"):\n",
        "    # Use gdown to download file from Google Drive\n",
        "    !gdown --fuzzy -O \"/content/kohya-trainer/checkpoint/{checkpoint_name}.ckpt\" \"{url}\"\n",
        "  elif url.startswith(\"magnet:?\"):\n",
        "    install_aria()\n",
        "    # Use aria2c to download file from magnet link\n",
        "    !aria2c --summary-interval=10 -c -x 10 -k 1M -s 10 -o {dst} \"{url}\"\n",
        "  else:\n",
        "    user_token = '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'\n",
        "    user_header = f\"\\\"Authorization: Bearer {user_token}\\\"\"\n",
        "    # Use wget to download file from URL\n",
        "    !wget -c --header={user_header} \"{url}\" -O {dst}\n",
        "\n",
        "def install_checkpoint():\n",
        "  # Iterate through list of models to install\n",
        "  for model in installModels:\n",
        "    # Call install function for each model\n",
        "    install(model[0], model[1])\n",
        "\n",
        "  if vaeName != \"none\":\n",
        "    for vae in installVae:\n",
        "      install(vae[0], vae[1])\n",
        "  else:\n",
        "    pass\n",
        "\n",
        "# Call install_checkpoint function to download all models in the list\n",
        "install_checkpoint()\n"
      ],
      "metadata": {
        "id": "SoucgZQ6jgPQ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0fzmhtywk_u"
      },
      "source": [
        "# Prepare Cloud Storage (Huggingface/GDrive)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "cwIJdhEcwk_u"
      },
      "outputs": [],
      "source": [
        "#@title Login to Huggingface hub\n",
        "\n",
        "#@markdown ## Instructions:\n",
        "#@markdown 1. Of course, you need a Huggingface account first.\n",
        "#@markdown 2. To create a huggingface token, go to `Profile > Access Tokens > New Token > Create a new access token` with the `Write` role.\n",
        "\n",
        "%cd /content/kohya-trainer\n",
        "\n",
        "from huggingface_hub import login\n",
        "login()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "jVgHUUK_wk_v"
      },
      "outputs": [],
      "source": [
        "#@title Mount Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "mount_drive = True #@param {'type':'boolean'}\n",
        "\n",
        "if mount_drive:\n",
        "  drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preparing Datasets"
      ],
      "metadata": {
        "id": "Pz9A2bu1Cq73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create train and reg folder \n",
        "# Import the os and shutil modules\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Change the current working directory to /content\n",
        "%cd /content\n",
        "\n",
        "# Define the dreambooth_directory variable\n",
        "dreambooth_directory = \"/content/dreambooth\"\n",
        "\n",
        "# Check if the dreambooth directory already exists\n",
        "if os.path.isdir(dreambooth_directory):\n",
        "  # If the directory exists, do nothing\n",
        "  pass\n",
        "else:\n",
        "  # If the directory does not exist, create it\n",
        "  os.mkdir(dreambooth_directory)\n",
        "\n",
        "#@markdown ### Define the reg_folder variable\n",
        "reg_count = 1 #@param {type: \"integer\"}\n",
        "reg_class =\"1girl\" #@param {type: \"string\"}\n",
        "reg_folder = str(reg_count) + \"_\" + reg_class\n",
        "\n",
        "# Define the reg_directory variable\n",
        "reg_directory = f\"{dreambooth_directory}/reg_{reg_class}\"\n",
        "\n",
        "# Check if the reg directory already exists\n",
        "if os.path.isdir(reg_directory):\n",
        "  # If the directory exists, do nothing\n",
        "  pass\n",
        "else:\n",
        "  # If the directory does not exist, create it\n",
        "  os.mkdir(reg_directory)\n",
        "\n",
        "# Define the reg_folder_directory variable\n",
        "reg_folder_directory = f\"{reg_directory}/{reg_folder}\"\n",
        "\n",
        "# Check if the reg_folder directory already exists\n",
        "if os.path.isdir(reg_folder_directory):\n",
        "  # If the directory exists, do nothing\n",
        "  pass\n",
        "else:\n",
        "  # If the directory does not exist, create it\n",
        "  os.mkdir(reg_folder_directory)\n",
        "\n",
        "#@markdown ### Define the train_folder variable\n",
        "train_count = 20 #@param {type: \"integer\"}\n",
        "train_token = \"makora\" #@param {type: \"string\"}\n",
        "train_class = \"1girl\" #@param {type: \"string\"}\n",
        "train_folder = str(train_count) + \"_\" + train_token + \" \" + train_class\n",
        "\n",
        "# Define the train_directory variable\n",
        "train_directory = f\"{dreambooth_directory}/train_{train_class}\"\n",
        "\n",
        "# Check if the train directory already exists\n",
        "if os.path.isdir(train_directory):\n",
        "  # If the directory exists, do nothing\n",
        "  pass\n",
        "else:\n",
        "  # If the directory does not exist, create it\n",
        "  os.mkdir(train_directory)\n",
        "\n",
        "# Define the train_folder_directory variable\n",
        "train_folder_directory = f\"{train_directory}/{train_folder}\"\n",
        "\n",
        "# Check if the train_folder directory already exists\n",
        "if os.path.isdir(train_folder_directory):\n",
        "  # If the directory exists, do nothing\n",
        "  pass\n",
        "else:\n",
        "  # If the directory does not exist, create it\n",
        "  os.mkdir(train_folder_directory)\n",
        "\n",
        "  \n"
      ],
      "metadata": {
        "id": "-CVfXAJMSqRi",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Booru Scraper\n",
        "#@markdown Use gallery-dl to scrape images from a booru site using the specified tags\n",
        "\n",
        "%cd /content\n",
        "\n",
        "# Set configuration options\n",
        "booru = \"Danbooru\" #@param [\"\", \"Danbooru\", \"Gelbooru\"]\n",
        "tag1 = \"hito_komoru\" #@param {type: \"string\"}\n",
        "tag2 = \"\" #@param {type: \"string\"}\n",
        "\n",
        "# Construct the search query\n",
        "if tag2 != \"\":\n",
        "  tags = tag1 + \"+\" + tag2\n",
        "else:\n",
        "  tags = tag1\n",
        "\n",
        "# Scrape images from the specified booru site using the given tags\n",
        "if booru.lower() == \"danbooru\":\n",
        "  !gallery-dl \"https://danbooru.donmai.us/posts?tags={tags}\" -D {train_folder_directory}\n",
        "elif booru.lower() == \"gelbooru\":\n",
        "  !gallery-dl \"https://gelbooru.com/index.php?page=post&s=list&tags={tags}\" -D {train_folder_directory}\n",
        "else:\n",
        "  print(f\"Unknown booru site: {booru}\")\n"
      ],
      "metadata": {
        "id": "Kt1GzntK_apb",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download compressed (.zip) dataset (Optional)\n",
        "\n",
        "\n",
        "#@markdown ### Define Download Parameter\n",
        "datasets_url = \"https://huggingface.co/datasets/Linaqruf/hitokomoru-tag/resolve/30e18a12b2e42dfe0b9252d85a36ae32251981d4/train_data.zip\" #@param {'type': 'string'}\n",
        "dataset_dst = '/content/train_data.zip' #@param{'type':'string'}\n",
        "#@markdown ### Define Auto-Unzip Parameter\n",
        "extract_to = '/content/kohya-trainer/train_data' #@param{'type':'string'}\n",
        "unzip_module = \"use_7zip\" #@param [\"use_unzip\",\"use_7zip\",\"use_Zipfile\"]\n",
        "\n",
        "def download_and_unzip_dataset(url, zip_file, extract_to, unzip_module):\n",
        "  try:\n",
        "    # Download dataset\n",
        "    if url.startswith(\"https://drive.google.com\"):\n",
        "      # Use gdown to download file from Google Drive\n",
        "      !gdown -o \"{zip_file}\" --fuzzy \"{url}\"\n",
        "    elif url.startswith(\"magnet:?\"):\n",
        "      install_aria()\n",
        "      # Use aria2c to download file from magnet link\n",
        "      !aria2c --summary-interval=10 -c -x 10 -k 1M -s 10 -o \"{zip_file}\" \"{url}\"\n",
        "    else:\n",
        "      user_token = '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'\n",
        "      user_header = f\"\\\"Authorization: Bearer {user_token}\\\"\"\n",
        "      # Use wget to download file from URL\n",
        "      !wget -c -O \"{zip_file}\" --header={user_header} \"{url}\"\n",
        "\n",
        "    # Unzip dataset\n",
        "    if unzip_module == \"use_7zip\":\n",
        "      !7z x \n",
        "extract_to\n",
        "    elif unzip_module == \"use_unzip\":\n",
        "      !unzip \n",
        "extract_to\n",
        "    elif unzip_module == \"use_Zipfile\":\n",
        "      import zipfile\n",
        "      with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "  except Exception as e:\n",
        "    print(\"An error occurred while downloading or unzipping the file:\", e)\n",
        "\n",
        "# Call download_and_unzip_dataset function\n",
        "download_and_unzip_dataset(datasets_url, dataset_dst, extract_to, unzip_module)\n",
        "     "
      ],
      "metadata": {
        "cellView": "form",
        "id": "Po6HdriyM7oC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Prepare Regularization Image\n",
        "prompt = \"1girl, solo\" #@param {type: \"string\"}\n",
        "negative = \"lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry\" #@param {type: \"string\"}\n",
        "model = \"/content/kohya-trainer/checkpoint/Anything-V3.0-pruned.ckpt\" #@param {type: \"string\"}\n",
        "vae = \"/content/kohya-trainer/checkpoint/anime.vae.pt\" #@param {type: \"string\"}\n",
        "reg_path = \"/content/dreambooth/reg_1girl/1_1girl\" #@param {type: \"string\"}\n",
        "scale = 12 #@param {type: \"slider\", min: 1, max: 40}\n",
        "sampler = \"ddim\" #@param [\"ddim\", \"pndm\", \"lms\", \"euler\", \"euler_a\", \"heun\", \"dpm_2\", \"dpm_2_a\", \"dpmsolver\",\"dpmsolver++\", \"dpmsingle\", \"k_lms\", \"k_euler\", \"k_euler_a\", \"k_dpm_2\", \"k_dpm_2_a\"]\n",
        "steps = 20 #@param {type: \"slider\", min: 1, max: 100}\n",
        "precision = \"fp16\" #@param [\"fp16\", \"bf16\"] {allow-input: false}\n",
        "width = 768 #@param {type: \"integer\"}\n",
        "height = 768 #@param {type: \"integer\"}\n",
        "batch_size = 4 #@param {type: \"integer\"}\n",
        "clip_skip = 2 #@param {type: \"slider\", min: 1, max: 40}\n",
        "\n",
        "train_num_images = sum(os.path.isfile(os.path.join(train_folder_directory, name)) for name in os.listdir(train_folder_directory))\n",
        "print(\"You have \" + str(train_num_images) + \" training data.\")\n",
        "\n",
        "if reg_count == 0:\n",
        "  reg_num_images = 0\n",
        "elif train_num_images > 0:\n",
        "  reg_num_images = sum(os.path.isfile(os.path.join(reg_folder_directory, name)) for name in os.listdir(reg_folder_directory))\n",
        "  print(\"You have \" + str(reg_num_images) + \" regularization images.\")\n",
        "  reg_num_images = (train_count * train_num_images) // reg_count - reg_num_images\n",
        "  print(\"You need \" + str(reg_num_images) + \" regularization images.\")\n",
        "  print(\"This process will generate \" + str(reg_num_images) + \" images left and place them in your regularization image path.\")\n",
        "\n",
        "!python /content/kohya-trainer/gen_img_diffusers/gen_img_diffusers.py \\\n",
        "  --ckpt {model} \\\n",
        "  --outdir {reg_path} \\\n",
        "  --xformers \\\n",
        "  --vae {vae} \\\n",
        "  --{precision} \\\n",
        "  --W {width} \\\n",
        "  --H {height} \\\n",
        "  --clip_skip {clip_skip} \\\n",
        "  --scale {scale} \\\n",
        "  --sampler {sampler} \\\n",
        "  --steps {steps} \\\n",
        "  --max_embeddings_multiples 3 \\\n",
        "  --batch_size {batch_size} \\\n",
        "  --images_per_prompt {reg_num_images} \\\n",
        "  --prompt \"{prompt} --n {negative}\"\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "HHjTV_1HKRbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing Training Data"
      ],
      "metadata": {
        "id": "d-9eJhB_QnBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Dataset Labeling\n",
        "%cd /content/kohya-trainer\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "use_blip_captioning = False #@param {type :'boolean'}\n",
        "use_wd_1_4_tagger = True #@param {type :'boolean'}\n",
        "\n",
        "global_batch_size = 8 #@param {type:'integer'}\n",
        "if use_blip_captioning:\n",
        "  def clone_and_prepare_spaces():\n",
        "    \"\"\"\n",
        "    Clones the Spaces repository, downloads the BLIP model weights, and moves the make_captions.py script to the BLIP directory.\n",
        "    \"\"\"\n",
        "    # Constants\n",
        "    BLIP_WEIGHT_SOURCE_URL = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth'\n",
        "    BLIP_WEIGHT_DESTINATION_PATH = '/content/kohya-trainer/BLIP/model_large_caption.pth'\n",
        "    MAKE_CAPTION_SOURCE_PATH = '/content/kohya-trainer/diffuser_fine_tuning/make_captions.py'\n",
        "    MAKE_CAPTION_DESTINATION_PATH = '/content/kohya-trainer/BLIP/make_captions.py'\n",
        "\n",
        "    # Install Git LFS\n",
        "    !git lfs install\n",
        "\n",
        "    # Clone the Spaces repository\n",
        "    !git clone https://huggingface.co/spaces/Salesforce/BLIP\n",
        "\n",
        "    # Download the BLIP model weights\n",
        "    !wget -c {BLIP_WEIGHT_SOURCE_URL} -O {BLIP_WEIGHT_DESTINATION_PATH}\n",
        "\n",
        "    # Move the make_captions.py script to the BLIP directory\n",
        "    if os.path.exists(MAKE_CAPTION_SOURCE_PATH):\n",
        "        shutil.move(MAKE_CAPTION_SOURCE_PATH, MAKE_CAPTION_DESTINATION_PATH)\n",
        "    else:\n",
        "        pass\n",
        "\n",
        "  # Clone and prepare Spaces\n",
        "  clone_and_prepare_spaces()\n",
        "\n",
        "  %cd /content/kohya-trainer/BLIP\n",
        "\n",
        "  caption_weights = \"model_large_caption.pth\"\n",
        "\n",
        "  !python make_captions.py \\\n",
        "    \"{train_folder_directory}\" \\\n",
        "    {caption_weights} \\\n",
        "    --batch_size {global_batch_size} \\\n",
        "    --caption_extension .caption\n",
        "else:\n",
        "  pass\n",
        "\n",
        "if use_wd_1_4_tagger:\n",
        "  # Change the working directory to the weight directory\n",
        "  %cd /content/kohya-trainer/diffuser_fine_tuning\n",
        "\n",
        "  !python tag_images_by_wd14_tagger.py \\\n",
        "    \"{train_folder_directory}\" \\\n",
        "    --batch_size {global_batch_size} \\\n",
        "    --caption_extension .txt\n",
        "else:\n",
        "  pass\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "nvPyH-G_Qdha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Datasets cleaner\n",
        "#@markdown This will delete unnecessary files and unsupported media like `.mp4`, `.webm`, and `.gif`\n",
        "\n",
        "%cd /content\n",
        "\n",
        "import os\n",
        "\n",
        "folder_target = \"/content/dreambooth/train_1girl/20_makora 1girl\" #@param {'type' : 'string'}\n",
        "\n",
        "test = os.listdir(folder_target)\n",
        "\n",
        "#@markdown I recommend to `keep_metadata` especially if you're doing resume training and you have metadata and bucket latents file from previous training like `.npz`, `.txt`, and `.caption`.\n",
        "keep_metadata = True #@param {'type':'boolean'}\n",
        "\n",
        "# List of supported file types\n",
        "if keep_metadata == True:\n",
        "  supported_types = [\".jpg\", \".jpeg\", \".png\", \".caption\", \".npz\", \".txt\"]\n",
        "else:\n",
        "  supported_types = [\".jpg\", \".jpeg\", \".png\"]\n",
        "\n",
        "# Iterate over all files in the directory\n",
        "for item in test:\n",
        "    # Extract the file extension from the file name\n",
        "    file_ext = os.path.splitext(item)[1]\n",
        "    # If the file extension is not in the list of supported types, delete the file\n",
        "    if file_ext not in supported_types:\n",
        "        # Print a message indicating the name of the file being deleted\n",
        "        print(f\"Deleting file {item} from {folder_target}\")\n",
        "        # Delete the file\n",
        "        os.remove(os.path.join(folder_target, item))\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Ug648uiOvUZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start Training\n",
        "\n"
      ],
      "metadata": {
        "id": "yHNbl3O_NSS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Training begin\n",
        "num_cpu_threads_per_process = 8 #@param {'type':'integer'}\n",
        "save_state = True #@param {'type':'boolean'}\n",
        "pre_trained_model_path =\"/content/kohya-trainer/checkpoint/Anything-V3.0-pruned.ckpt\" #@param {'type':'string'}\n",
        "train_data_dir = \"/content/drive/MyDrive/dreambooth/train_1girl\" #@param {'type':'string'}\n",
        "reg_data_dir = \"/content/drive/MyDrive/dreambooth/reg_1girl\" #@param {'type':'string'}\n",
        "output_dir = \"/content/drive/MyDrive/dreambooth\" #@param {'type':'string'}\n",
        "resume_path =\"\" #@param {'type':'string'}\n",
        "train_batch_size = 1  #@param {type: \"slider\", min: 1, max: 10}\n",
        "resolution = \"512\" #@param [\"512\", \"768\"] {allow-input: false}\n",
        "learning_rate =\"5e-7\" #@param {'type':'string'}\n",
        "mixed_precision = \"fp16\" #@param [\"fp16\", \"bf16\"] {allow-input: false}\n",
        "max_train_steps = 10000 #@param {'type':'integer'}\n",
        "save_precision = \"fp16\" #@param [\"float\", \"fp16\", \"bf16\"] {allow-input: false}\n",
        "save_every_n_epochs = 50 #@param {'type':'integer'}\n",
        "caption_extension = \".txt\" #@param [\"none\", \".caption\", \".txt\"] {allow-input: false}\n",
        "\n",
        "#@markdown ### Log And Debug\n",
        "log_prefix = \"dreambooth-style1\" #@param {'type':'string'}\n",
        "logs_dst = \"/content/kohya-trainer/logs\" #@param {'type':'string'}\n",
        "debug_mode = False #@param {'type':'boolean'}\n",
        "\n",
        "if debug_mode == True:\n",
        "  debug_dataset = \"--debug_dataset\"\n",
        "else:\n",
        "  debug_dataset = \"\"\n",
        "\n",
        "if save_state == True:\n",
        "  sv_state = \"--save_state\"\n",
        "else:\n",
        "  sv_state = \"\"\n",
        "\n",
        "if resume_path != \"\":\n",
        "  rs_state = \"--resume \" + str(resume_path)\n",
        "else:\n",
        "  rs_state = \"\"\n",
        "\n",
        "if caption_extension != \"none\":\n",
        "  captions_n_tags = \"--caption_extension =\" + str(caption_extension)\n",
        "  shuffle =\"--shuffle_caption\"\n",
        "else:\n",
        "  captions_n_tags = \"\"\n",
        "  shuffle = \"\"\n",
        "\n",
        "  \n",
        "%cd /content/kohya-trainer/train_db_fixed\n",
        "!accelerate launch \\\n",
        "    --config_file /content/kohya-trainer/accelerate_config/config.yaml \\\n",
        "    --num_cpu_threads_per_process {num_cpu_threads_per_process} \\\n",
        "    train_db_fixed.py \\\n",
        "    --pretrained_model_name_or_path={pre_trained_model_path} \\\n",
        "    --train_data_dir={train_data_dir} \\\n",
        "    --reg_data_dir={reg_data_dir} \\\n",
        "    --output_dir={output_dir} \\\n",
        "    --prior_loss_weight=1.0 \\\n",
        "    --resolution={resolution} \\\n",
        "    --save_precision {save_precision} \\\n",
        "    --train_batch_size={train_batch_size}\\\n",
        "    --learning_rate={learning_rate}\\\n",
        "    --max_train_steps={max_train_steps}  \\\n",
        "    --use_8bit_adam \\\n",
        "    --xformers \\\n",
        "    --mixed_precision={mixed_precision} \\\n",
        "    --gradient_checkpointing \\\n",
        "    --save_every_n_epochs={save_every_n_epochs} \\\n",
        "    --enable_bucket \\\n",
        "    --cache_latents \\\n",
        "    {shuffle} \\\n",
        "    {debug_dataset} \\\n",
        "    {captions_n_tags} \\\n",
        "    {sv_state} \\\n",
        "    {rs_state} \\\n",
        "    --logging_dir={logs_dst} \\\n",
        "    --log_prefix {log_prefix}\n"
      ],
      "metadata": {
        "id": "X_Rd3Eh07xlA",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EaUgsPtW07NZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/kohya-trainer/convert_diffusers20_original_sd\n",
        "\n",
        "#@title Convert Weight to Diffusers or `.ckpt/.safetensors` (Optional)\n",
        "#@markdown ## Define weight path\n",
        "weight = \"/content/kohya-trainer/fine-tuned/model.ckpt\" #@param {'type': 'string'}\n",
        "weight_dir = os.path.dirname(weight)\n",
        "convert = \"diffusers_to_ckpt_safetensors\" #@param [\"diffusers_to_ckpt_safetensors\", \"ckpt_safetensors_to_diffusers\"] {'allow-input': false}\n",
        "\n",
        "#@markdown ## Conversion Config\n",
        "#@markdown\n",
        "#@markdown ### Diffusers to `.ckpt/.safetensors`\n",
        "use_safetensors = True #@param {'type': 'boolean'}\n",
        "\n",
        "if use_safetensors:\n",
        "    checkpoint = str(weight_dir)+\"/model.safetensors\"\n",
        "else:\n",
        "    checkpoint = str(weight_dir)+\"/model.ckpt\"\n",
        "\n",
        "save_precision = \"--float\" #@param [\"--fp16\",\"--bf16\",\"--float\"] {'allow-input': false}\n",
        "\n",
        "#@markdown ### `.ckpt/.safetensors` to Diffusers\n",
        "#@markdown is your model v1 or v2 based Stable Diffusion Model\n",
        "version = \"--v1\" #@param [\"--v1\",\"--v2\"] {'allow-input': false}\n",
        "diffusers = str(weight_dir)+\"/diffusers_model\"\n",
        "\n",
        "#@markdown Add reference model to get scheduler, optimizer, and tokenizer, because `.ckpt/.safetensors` didn't have one.\n",
        "reference_model =\"runwayml/stable-diffusion-v1-5\" #@param {'type': 'string'}\n",
        "\n",
        "if convert == \"diffusers_to_ckpt_safetensors\":\n",
        "    if not weight.endswith(\".ckpt\") or weight.endswith(\".safetensors\"):\n",
        "        !python convert_diffusers20_original_sd.py \\\n",
        "            {weight} \\\n",
        "            {checkpoint} \\\n",
        "            {save_precision}\n",
        "\n",
        "else:    \n",
        "    !python convert_diffusers20_original_sd.py \\\n",
        "        {weight} \\\n",
        "        {diffusers} \\\n",
        "        {version} \\\n",
        "        --reference_model {reference_model} "
      ],
      "metadata": {
        "cellView": "form",
        "id": "HPbZ0E2uNYYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Model Pruner (Optional)\n",
        "\n",
        "##Lopho\n",
        "\n",
        "#@markdown Do you want to Prune a model?\n",
        "%cd /content/ \n",
        "\n",
        "# Use a more descriptive variable name\n",
        "prune = False #@param {'type':'boolean'}\n",
        "\n",
        "# Add a checkbox to enable/disable the `--fp16` argument\n",
        "fp16 = False #@param {'type':'boolean'}\n",
        "\n",
        "# Add a checkbox to enable/disable the `--ema` argument\n",
        "ema = False #@param {'type':'boolean'}\n",
        "\n",
        "# Add a checkbox to enable/disable the `--no-clip` argument\n",
        "no_clip = False #@param {'type':'boolean'}\n",
        "\n",
        "# Add a checkbox to enable/disable the `--no-vae` argument\n",
        "no_vae = False #@param {'type':'boolean'}\n",
        "\n",
        "# Use a more descriptive variable name\n",
        "input = \"/content/kohya-trainer/fine_tuned/last.ckpt\" #@param {'type' : 'string'}\n",
        "\n",
        "# Use a more descriptive variable name\n",
        "output = \"/content/kohya-trainer/fine_tuned/last-pruned.ckpt\" #@param {'type' : 'string'}\n",
        "\n",
        "if prune:\n",
        "  import os\n",
        "  if os.path.isfile('/content/prune.py'):\n",
        "    pass\n",
        "  else:\n",
        "    # Add a comment to explain what the code is doing\n",
        "    # Download the pruning script if it doesn't already exist\n",
        "    !wget https://raw.githubusercontent.com/lopho/stable-diffusion-prune/main/prune.py\n",
        "\n",
        "\n",
        "# Add a comment to explain what the code is doing\n",
        "# Run the pruning script with the specified arguments\n",
        "!python3 prune.py {input} {output} {'--fp16' if fp16 else ''} {'--ema' if ema else ''} {'--no-clip' if no_clip else ''} {'--no-vae' if no_vae else ''}\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Yiskg-mZX0Ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jypUkLWc48R_"
      },
      "source": [
        "## Commit trained model to Huggingface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvZgRSmKVSRw"
      },
      "source": [
        "### To Commit models:\n",
        "1. Create a huggingface repository for your model.\n",
        "2. Clone your model to this Colab session.\n",
        "3. Move the necessary files to your repository to save your trained model to huggingface. These files are located in `fine-tuned` folder:\n",
        "   - `epoch-nnnnn.ckpt` and/or\n",
        "   - `last.ckpt`\n",
        "4. Commit your model to huggingface.\n",
        "\n",
        "### To Commit datasets:\n",
        "1. Create a huggingface repository for your datasets.\n",
        "2. Clone your datasets to this Colab session.\n",
        "3. Move the necessary files to your repository so that you can resume training without rebuilding your dataset with this notebook.\n",
        "  - The `train_folder` folder.\n",
        "4. Commit your datasets to huggingface.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "182Law9oUiYN"
      },
      "outputs": [],
      "source": [
        "#@title Clone Model or Datasets\n",
        "\n",
        "#@markdown Opt-out this cell when run all\n",
        "opt_out = True #@param {'type':'boolean'}\n",
        "\n",
        "#@markdown Type of item to clone (model or dataset)\n",
        "type_of_item = \"model\" #@param [\"model\", \"dataset\"]\n",
        "\n",
        "#@markdown Install or uninstall git lfs\n",
        "install_git_lfs = False #@param {'type':'boolean'}\n",
        "\n",
        "if opt_out == False:\n",
        "  %cd /content\n",
        "  username = \"your-huggingface-username\" #@param {'type': 'string'}\n",
        "  model_repo = \"your-huggingface-model-repo\" #@param {'type': 'string'}\n",
        "  datasets_repo = \"your-huggingface-datasets-repo\" #@param {'type': 'string'}\n",
        "  \n",
        "  if type_of_item == \"model\":\n",
        "    Repository_url = f\"https://huggingface.co/{username}/{model_repo}\"\n",
        "  elif type_of_item == \"dataset\":\n",
        "    Repository_url = f\"https://huggingface.co/datasets/{username}/{datasets_repo}\"\n",
        "\n",
        "  if install_git_lfs:\n",
        "    !git lfs install\n",
        "  else:\n",
        "    !git lfs uninstall\n",
        "\n",
        "  !git clone {Repository_url}\n",
        "else:\n",
        "  pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "87wG7QIZbtZE"
      },
      "outputs": [],
      "source": [
        "#@title Commit Model or Datasets to Huggingface\n",
        "\n",
        "#@markdown Opt-out this cell when run all\n",
        "opt_out = True #@param {'type':'boolean'}\n",
        "\n",
        "#@markdown Type of item to commit (model or dataset)\n",
        "type_of_item = \"model\" #@param [\"model\", \"dataset\"]\n",
        "\n",
        "if opt_out == False:\n",
        "  %cd /content\n",
        "  #@markdown Go to your model or dataset path\n",
        "  item_path = \"your-cloned-model-or-datasets-repo\" #@param {'type': 'string'}\n",
        "\n",
        "  #@markdown #Git Commit\n",
        "\n",
        "  #@markdown Set **git commit identity**\n",
        "  email = \"your-email\" #@param {'type': 'string'}\n",
        "  name = \"your-username\" #@param {'type': 'string'}\n",
        "  #@markdown Set **commit message**\n",
        "  commit_m = \"feat: upload 6 epochs model\" #@param {'type': 'string'}\n",
        "\n",
        "  %cd {item_path}\n",
        "  !git lfs install\n",
        "  !huggingface-cli lfs-enable-largefiles .\n",
        "  !git add .\n",
        "  !git lfs help smudge\n",
        "  !git config --global user.email \"{email}\"\n",
        "  !git config --global user.name \"{name}\"\n",
        "  !git commit -m \"{commit_m}\"\n",
        "  !git push\n",
        "\n",
        "else:\n",
        "  pass"
      ]
    }
  ]
}